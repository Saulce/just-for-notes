# PYTHON 爬虫

​	支持：PyC、Jupyter、Anaconda、Spyder

#### 列表 list

变量名 = []

当获取多个数据时，可以将它们存储到列表中，直接访问列表使用

#### 元组 tuple

变量名 = ()

#### 字典 dict

变量名 = {属性:值, …}

scrapy框架 使用 “重要”

**逻辑运算符性能提升** and / or : 当and前面的表达式的结果为 true 才执行后面的表达式

### 格式化输出

​	scrapy框架——excel mysql redis

%s 代表字符串	%d 代表数值

输出print(' %s + %d ' % (..., ...))	输入input( )

## 常用操作

### 字符串

len

find

startswitch, endswitch

count

replace

split

upper, lower

strip

join

### 列表

​	爬取到的内容放入列表中

增：

1. append 末尾添加元素 ( o )
2. insert 指定位置插入元素 (index, o)
3. extend 合并两个列表

删：

1. del 根据下标删除
2. pop 删除最后一个元素
3. remove 根据元素的值删除

改：通过下标

查：

1. in 判断某元素是否在列表中
2. not in 反上

### 元组高级

​	元组与列表类似，但元组的元素不能修改；定义只有一个元素的元组，需要在唯一的元素后写上一个逗号，否则将成为整型而非元组

切片（字符串、列表、元组都支持）：[ 起始 : 结束 (不含此下标) : 步长 (默认为1) ]

### 字典

增： 变量名['key'] = 值 （key原先不存在）

删：

1. del (1) 删除指定的某一元素 (2) 删除整个字典对象
2. clear 清空字典保留字典对象

改： 通过 ['key'] 修改

查：

- 通过['key'] 查看，不能使用 '.' 直接查询，但可以使用 .get('key')
- 通过[ ] 获取不存在的key的error，.get()则会返回none

遍历：

1. key  字典名.keys()
2. value  字典名.values()
3. key和value  字典名.items() 用key和value for
4. 项  字典名.items() 用一个变量接收

### 文件

f = open(文件路径，访问模式)	可创建文件，但不可以创建文件夹

f.write('值')	打开文件 -> 若文件存在，会先清空原先的内容再写入	访问模式 a——可解决清空

f.close() 	关闭文件

content = f.read()	读数据 -> 默认单个字节读，效率慢——f.readlin() / readlines()

print(content)

### 序列化和反序列化

只有将对象进行序列化才能将它写入文件中，否则只能写入字符串。

JSON模块实现——import json  导入json模块

对象（内存中的数据）<--> 字节序列

#### 两种方式序列化

json.dumps(列表)	类型转换list -> str

json.dump(列表, 文件)	将对象转化为字符串的同时写入到一个文件的对象中；解释：在使用scrapy框架时会返回一个对象，要将对象写入文件，就要用json

#### 反序列化

json.loads()	json.load( , )

## 异常

异常抛出：

​	`try:`

​		`可能出现异常的代码表达式`

​	`except 异常的类型`

​		`友好的提示`

- HTTPError是URLError的子类。http错误是针对浏览器无法连接到服务器而增加出来的错误提示

​	import urllib.error

​	except urllib.error.HTTPError:

​		print(' ')

- url错误是主机地址或参数出现错误

​	except urllib.error.URLError

## 反爬手段

robots协议

1. User-Agent
2. 代理ip
3. 验证码访问
4. 动态加载网页
5. 数据加密

## urllib库

​	获取网址源码

引入：import urllib.request

response = urllib.request.urlopen('url')  模拟浏览器向服务器发送请求

content = response.read()  read方法返回字节形式的二进制数据 -> 字符串（解码） / 

content = response.read().decode('编码格式')

print(content)

### 一个类型六个方法

response是HTTPResponse的类型

- .read() 逐字节读取
- .readline() 一行读取
- .readlines() 多行读取
- .getcode() 返回状态码，200 证明逻辑没错
- .geturl() 返回地址url
- .getheaders() 获取响应头

### cookie登录

​	数据采集时需要绕过登录；需要关注页面编码格式；cookie中携带着登录信息

referer 判断当前路径是否由上个路径进入，一般做图片防盗链

1. 一般看不到的数据都是字啊页面的源码重，所以需要获取页面源码然后进行解析（反爬：隐藏域）

2. 验证码：

   有坑！零次request请求不一样导致验证码不同，使用session()的返回值使请求变成同一个对象

## handler 处理器

### 代理服务器

proxies = {'' : ''}

handler代码 不同点：handler = urllib.request.ProxyHandler(proxies = proxies)

#### 代理池

proxies_pool = [{},{},...]

### xpath解析

## Selenium

### 元素定位

### 访问元素信息

### 封装的handless

from selenium import webdriver

from selenium.webdriver.chrome.options import Options

## Requests

一个类型六个属性

response 类型：models.Response

- request.text：获取网站源码
- request.encoding：访问或定制编码方式
- request.url：获取请求的url
- request.content：响应的字节类型
- request.status_code：响应的状态码
- request.headers：响应的头信息

response的属性和方法

- response.text  获取响应的字符串
- response.body  获取二进制数据
- response.xpath  直接使用xpath解析
- response.extract()  提取selector对象的data属性值
- response.extract_first()  提取selector列表的第一个数据

## Scrapy

### scrapy项目的结构：

项目名称

​	项目名称

​		spiders文件夹（存储的是爬虫文件）

​			init

​			items（定义数据结构的地方）

​			middleware（中间件 代理）

​			pipelines（管道 用来处理下载的数据）

​			settings（配置文件 ：robots协议、UA定义等）

​			init

​			自定义的爬虫文件（核心功能文件）

### scrapy的架构组成

1. 引擎
2. 下载器
3. spiders
4. 调度器
5. 管道

### scrapy的工作原理

#### 借助ipython使用 scrapy shell调试

## CrawlSpider

​	继承自scrapy.Spider

crawlerspider可以定义规则，在解析html内容时，可以根据链接规则提取出指定的链接，然后向这些链接发送请求。所以爬取了网页之后需要提取链接再次爬取，即可使用合适的

## 日志

级别：

- CRITICAL	严重错误
- ERROR            一般错误
- WARNING        警告
- INFO        一般信息
- DEBUG        调试信息

settings.py 文件设置：

​	默认级别为debug会显示上面所有的信息

​	LOG_FILE := .log文件，存储日志信息

​	LOG_LEVEL : 指定日志级别